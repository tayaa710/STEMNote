# Architecture (Source of Truth)

This document defines how the app is structured and how the AI "folder context" works.
Claude Code must follow this structure.

## 1) High-level system overview

User flow:
1. User draws on a page (handwritten notes).
2. User enters selection mode and lassos a region (the "target").
3. User asks a question about the target region.
4. System retrieves relevant context from the folder (other pages + PDFs).
5. AI answers and cites which pages were used.

Core idea: Retrieval Augmented Generation (RAG)
- We index each page/PDF into small text chunks.
- Each chunk has an embedding vector representing its meaning.
- On question, we retrieve top-K relevant chunks within the folder and send ONLY those
  chunks + the selected region image to the model.

## 2) Repo structure

root
- README.md
- ARCHITECTURE.md
- TASKS.md
- app/
  - src/
    - screens/
      - FolderListScreen.tsx
      - NoteListScreen.tsx
      - PageEditorScreen.tsx
    - components/
      - FolderRow.tsx
      - NoteRow.tsx
      - AskSheet.tsx
      - SelectionOverlay.tsx
    - drawing/
      - DrawingCanvas.tsx
      - drawingTypes.ts
      - render.ts (export page/region images)
    - storage/
      - db.ts (SQLite init)
      - foldersRepo.ts
      - notesRepo.ts
      - pagesRepo.ts
      - files.ts (paths, caching)
    - ai/
      - apiClient.ts (calls Supabase Edge Functions only)
      - rag.ts (client helpers: request indexing, request answers)
      - types.ts
    - utils/
      - ids.ts
      - logger.ts
  - ios/ (generated by RN)
  - android/ (not primary, but should still build)

## 3) App screens and responsibilities

### FolderListScreen
- lists folders
- create/rename/delete folders
- navigates to NoteListScreen(folderId)

### NoteListScreen
- lists notes in folder
- create note
- navigates to PageEditorScreen(noteId, pageIndex)

### PageEditorScreen
- drawing surface (Skia canvas)
- tools: pen, eraser, undo/redo, clear
- page save/load
- selection mode (lasso)
- AskSheet entry point ("Ask about selection")

### AskSheet
- shows:
  - question input
  - answer
  - citations list
- citations are tappable: jump to cited page

## 4) Local data model (MVP is local-only)

SQLite tables (local):
- folders
  - id TEXT PRIMARY KEY
  - name TEXT
  - created_at INTEGER
  - updated_at INTEGER

- notes
  - id TEXT PRIMARY KEY
  - folder_id TEXT
  - title TEXT
  - created_at INTEGER
  - updated_at INTEGER

- pages
  - id TEXT PRIMARY KEY
  - note_id TEXT
  - page_index INTEGER
  - drawing_json TEXT (MVP)
  - page_image_path TEXT (rendered PNG)
  - extracted_text TEXT (optional)
  - updated_at INTEGER

- indexing_jobs (optional)
  - id TEXT PRIMARY KEY
  - folder_id TEXT
  - page_id TEXT
  - status TEXT (queued|running|done|error)
  - error TEXT

Notes:
- Drawing storage format in MVP can be JSON describing strokes.
- We must be able to render:
  - full page image PNG (for indexing)
  - selected region PNG (for asking)

## 5) Backend (Supabase) responsibilities

All AI calls must go through Supabase Edge Functions. Never call OpenAI/Anthropic from the app.

Supabase (server) tables:
- chunks
  - id UUID
  - folder_id TEXT
  - source_type TEXT ('page'|'pdf')
  - source_id TEXT (pageId or pdfId)
  - page_index INTEGER (nullable for pdf if not per-page)
  - chunk_text TEXT
  - embedding VECTOR
  - metadata JSON (optional: noteId, title, etc.)
  - created_at TIMESTAMP

Edge Functions (server endpoints):
1) POST /askRegion
   - Input:
     - folderId: string
     - question: string
     - regionImageBase64: string
     - pageId?: string
   - Output:
     - answer: string
     - citations: { sourceType, sourceId, pageIndex?, noteId?, title? }[]
     - debug?: { retrievedChunkIds: string[] } (optional for dev)

2) POST /indexPage
   - Input:
     - folderId: string
     - noteId: string
     - pageId: string
     - pageImageBase64: string (or a storage path)
   - Behavior:
     - Extract text (OCR or multimodal transcription)
     - Chunk text
     - Compute embeddings
     - Upsert into chunks table for this page
   - Output:
     - ok: boolean
     - chunksUpserted: number

3) POST /indexPdf
   - Input:
     - folderId: string
     - pdfId: string
     - pdfTextByPage: { pageIndex: number, text: string }[]
   - Output:
     - ok: boolean

## 6) AI provider assignments (recommended)

MVP recommended split:
- Embeddings: OpenAI embeddings
- Image understanding: OpenAI multimodal (for region/page image)
- Final reasoning answer: Claude (optional) OR OpenAI (simplify)

Simplify-first option:
- Use OpenAI for everything in MVP (fewer moving parts).
Quality-first option:
- OpenAI for embeddings + image understanding
- Claude for final answer using retrieved text context

## 7) RAG details (how context is retrieved)

Indexing:
- For each page/PDF page:
  - get text (best-effort)
  - split into chunks (target: ~200-500 tokens)
  - embedding per chunk
  - store vectors in chunks table, scoped by folder_id

Answering:
- Build a query text: question + (optional) quick transcription of region
- Embed query
- Vector search within folder_id to get top-K chunks (K=8 default)
- Send to model:
  - selected region image (base64)
  - retrieved chunks text
  - instruction: explain + debug + cite sources

## 8) Security constraints
- No API keys in the client app.
- Supabase Edge Functions store keys in environment variables.
- Client only calls Supabase endpoints.

## 9) Performance constraints (MVP)
- Indexing runs on page save, not every stroke.
- Show "Indexingâ€¦" status per folder/page.
- Cache rendered page images locally.

## 10) Later upgrade path (not MVP)
- Add PencilKit bridge for higher-quality drawing.
- Add login/sync (Supabase auth + storage).
- Add offline-first sync conflict resolution.